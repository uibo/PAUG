{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4470a0-b5bd-4c77-9794-4fc9702b659c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/08 10:13:13 WARN Utils: Your hostname, boui resolves to a loopback address: 127.0.1.1; using 192.168.0.6 instead (on interface wlp1s0)\n",
      "24/06/08 10:13:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/boui/Desktop/github/PAUG/venv/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "24/06/08 10:13:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/08 10:13:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, to_date, col, array, when\n",
    "from pyspark.sql.types import IntegerType, ArrayType, StringType\n",
    "import re\n",
    "\n",
    "spark = SparkSession.builder.appName(\"processing\").getOrCreate()\n",
    "\n",
    "# 제품명_filtering.csv 읽어오기\n",
    "df = spark.read.csv(\"/home/boui/Desktop/usedGoods_data/rawdata/iphone14_filtering.csv\", header = True, multiLine=True, quote='\"', escape='\"')\n",
    "df = df.withColumn(\"feature_list\", array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d612ac0f-e408-4f5c-bb5f-2d35fa7db29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류할 model명, model을 위한 정규식 작성\n",
    "model1 = ('iPhone14', r'(?i)(?:iPhone|아이폰)\\s?14(?!(?:\\s?(?:Pro|Plus|Pro\\s?Max|프로|플러스|프로\\s?맥스)))')\n",
    "model2 = ('iPhone14Plus', r'(?i)(?:iPhone|아이폰)\\s?14\\s?(?:Plus|플러스)')\n",
    "model3 = ('iPhone14Pro', r'(?i)(?:iPhone|아이폰)\\s?14\\s?(?:Pro|프로)(?!(?:\\s?(?:Max|맥스)))')\n",
    "model4 = ('iPhone14ProMax', r'(?i)(?:iPhone|아이폰)\\s?14\\s?(?:Pro|프로)\\s?(?:Max|맥스)')\n",
    "model_list =[model1, model2, model3, model4]\n",
    "\n",
    "# {\"model명\" : \"model 정규식\"} dict생성\n",
    "model_dict = dict()\n",
    "for model in model_list:\n",
    "    model_dict[model[0]] = re.compile(model[1])\n",
    "\n",
    "# model 분류를 위한 spark.udf작성\n",
    "def classify_model(title):\n",
    "    for model in model_dict:\n",
    "        match = model_dict[model].search(title)\n",
    "        if match:\n",
    "            return model\n",
    "    return 'None'\n",
    "    \n",
    "classify_model_udf = udf(classify_model, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe05aa1-e9ac-4794-9cff-2b9b3f08c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage 분류를 위한 spark.udf작성\n",
    "def classify_storage_feature(feature_list, title, context):\n",
    "    pattern = re.compile(r'(?<!\\d)(64|128|256|512|1)(?!\\d)', re.IGNORECASE)\n",
    "    \n",
    "    # title에서 storage 추출\n",
    "    match = pattern.search(title)\n",
    "    if match:\n",
    "        if match.group(1) == '1':\n",
    "            feature_list.append(\"1024GB\")\n",
    "        else:\n",
    "            feature_list.append(match.group(1)+\"GB\")\n",
    "        return feature_list\n",
    "\n",
    "    # title에서 추출 못할 시 context에서 storage 추출\n",
    "    match = pattern.search(context)\n",
    "    if match:\n",
    "        if match.group(1) == '1':\n",
    "            feature_list.append(\"1024GB\")\n",
    "        else:\n",
    "            feature_list.append(match.group(1)+\"GB\")\n",
    "        return feature_list\n",
    "    return feature_list\n",
    "    \n",
    "classify_storage_feature_udf = udf(classify_storage_feature, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bc6b0f5-a4b1-4681-a651-9dd30aff6788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제품의 일반적인 특징을 뽑아내기 위한 spark.udf작성\n",
    "def extract_general_feature(feature_list, context):\n",
    "    # context 없는 경우 feature 추출하지 않고 바로 return\n",
    "    if context is None:\n",
    "        return feature_list\n",
    "    \n",
    "    # 제품이 갖고있는 특징들이 feature_list에 삽입되고 return됨\n",
    "    pattern = re.compile(r'.*미개봉.*')\n",
    "    if pattern.search(context):\n",
    "        feature_list.append('미개봉')\n",
    "        return feature_list\n",
    "\n",
    "    # 일반적인 특징을 뽑아내기 위해 정규식 구성에 사용될 단어 및 형태소\n",
    "    checking_feature_list = ['기스', '깨짐', '잔상', '흠집', '파손', '찍힘']\n",
    "    checking_morpheme_list = ['있', '존재', '정도']\n",
    "\n",
    "    for checking_feature in checking_feature_list:\n",
    "        for checking_morpheme in checking_morpheme_list:\n",
    "            pattern = re.compile(fr\".*{checking_feature}.{{0,10}}{checking_morpheme}.*\")\n",
    "            match = pattern.search(context)\n",
    "            \n",
    "            if match:\n",
    "                feature_list.append(checking_feature)\n",
    "                break\n",
    "                    \n",
    "    checking_feature_list2 = ['부품용']  \n",
    "    for checking_feature in checking_feature_list2:\n",
    "        if checking_feature in context:\n",
    "            feature_list.append(checking_feature)\n",
    "                \n",
    "    return feature_list\n",
    "    \n",
    "extract_general_feature_udf = udf(extract_general_feature, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6fd8835-dca2-41ac-8935-6df5e4ff3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 애플 제품 한정, 애플케어플러스를 추출하기 위한 spark.udf작성\n",
    "def extract_applecare_feature(feature_list, context):\n",
    "    # context 없는 경우 feature 추출하지 않고 바로 return\n",
    "    if context is None:\n",
    "        return feature_list\n",
    "        \n",
    "    pattern = re.compile(r\"(케어|캐어|애케|애캐|애플케어|애플캐어|애케플|애캐플).{0,15}(포함|까지|있|적용)\")\n",
    "    match = pattern.search(context)\n",
    "    if match:\n",
    "        feature_list.append('애플케어플러스')\n",
    "        return feature_list\n",
    "    return feature_list\n",
    "\n",
    "extract_applecare_feature_udf = udf(extract_applecare_feature, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44cbdcef-3825-4d7b-880d-f69155af3d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_battery(context):\n",
    "    # battery 값 확인할 수 없다는 의미로 -1\n",
    "    if context is None:\n",
    "        return -1\n",
    "        \n",
    "    pattern = re.compile(r'.*미개봉.*')\n",
    "    if pattern.search(context):\n",
    "        return 100\n",
    "        \n",
    "    pattern = re.compile(r\".*배터리.{0,7}(\\d{2,3}).{0,3}\\s*(퍼센트|프로|%|퍼)\")\n",
    "    match = pattern.search(context)\n",
    "    if match:\n",
    "        efficiency = match.group(1)\n",
    "        \n",
    "        if efficiency == '00':\n",
    "            efficiency = '100'\n",
    "        return int(efficiency)\n",
    "        \n",
    "    pattern = re.compile(r\".*배터리.{0,7}(\\d{2,3}).*\")\n",
    "    match = pattern.search(context)\n",
    "    if match:\n",
    "        efficiency = match.group(1)\n",
    "        \n",
    "        if efficiency == '00':\n",
    "            efficiency = '100'\n",
    "        return int(efficiency)\n",
    "    return -1\n",
    "\n",
    "\n",
    "extract_battery_udf = udf(extract_battery, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b297459-8bf7-4106-a097-a6b5d0f7fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('model', classify_model_udf(df.title))\n",
    "df = df.withColumn('feature_list', classify_storage_feature_udf(df.feature_list, df.title, df.context))\n",
    "df = df.withColumn('feature_list', extract_general_feature_udf(df.feature_list, df.context))\n",
    "df = df.withColumn('feature_list', extract_applecare_feature_udf(df.feature_list, df.context))\n",
    "df = df.withColumn('battery', extract_battery_udf(df.context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31d56e84-b82e-4332-b90c-8a115a081948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 모델 분류 안되는 행 삭제\n",
    "df = df.filter(col(\"model\") != \"None\")\n",
    "# location 채워 넣기\n",
    "df = df.withColumn(\"location\", when((df.location.isNull()) | (df.location == \"\"), \"-\").otherwise(df.location))\n",
    "df = df.select(['model', 'feature_list', 'battery', 'upload_date', 'price', 'status', 'location', 'imgUrl', 'url','context']).toPandas()\n",
    "df.to_csv(\"/home/boui/Desktop/usedGoods_data/cleandata/iphone14_cleaning.csv\",header=True, index = False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775348e8-38e0-4b96-87ae-e39e51f8507a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
